{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Plain DNN for magnetic field_forward problem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vantainguyen/Bayesian-optimization-/blob/main/1_Plain_DNN_for_magnetic_field_forward_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0kgGFQYEito"
      },
      "source": [
        "# Plain-DNN for magnetic field "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE4l7SXJFAIA"
      },
      "source": [
        "## Navigating to the working directory containing the datasets and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxJrdrvhWwM7"
      },
      "source": [
        "source_file = False\n",
        "\n",
        "if source_file:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  source_file = '/content/drive/My Drive/Datasets_for_magfield'\n",
        "\n",
        "else:\n",
        "  source_file = '/home/opc/vantai/Datasets_for_magfield'\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GULLKWdrGUvW"
      },
      "source": [
        "## Checking the current working GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPKdK-bUehpt",
        "outputId": "7a333387-9ab8-4822-dfb9-081c07987308"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 18 01:13:07 2021       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    34W / 300W |   6745MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      3585      C   ...opc/miniconda3/bin/python     6743MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx4vPkX4Gbkf"
      },
      "source": [
        "## Loading the datasets and doing feature engineering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SnpiTSbXdYI"
      },
      "source": [
        "# Loading data\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "os.chdir(source_file)\n",
        "\n",
        "with open('para_2m_e', 'rb') as file_para:\n",
        "    para = pickle.load(file_para)\n",
        "    \n",
        "with open('output_field_2m_e', 'rb') as file_out:\n",
        "    output_field = pickle.load(file_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn59wTchWvht",
        "outputId": "478b7f4c-183b-4e9c-d757-dbe72af4550e"
      },
      "source": [
        "len(output_field)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr65NU2CXxxL"
      },
      "source": [
        "# Feature engineering\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "random.seed(123)\n",
        "\n",
        "para_np = np.array(para)\n",
        "output_field_np = np.array(output_field)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(para_np, output_field_np, test_size=.20, shuffle=True)\n",
        "\n",
        "x_sScaler = StandardScaler()\n",
        "x_sScaler.fit(x_train)\n",
        "s_x_train_scaled = x_sScaler.transform(x_train)\n",
        "s_x_test_scaled = x_sScaler.transform(x_test)\n",
        "\n",
        "y_sScaler = StandardScaler()\n",
        "y_sScaler.fit(y_train)\n",
        "s_y_train_scaled = y_sScaler.transform(y_train)\n",
        "s_y_test_scaled = y_sScaler.transform(y_test)\n",
        "\n",
        "\n",
        "# Saving training, testing and scalers data\n",
        "\n",
        "\n",
        "with open('re_s_x_train_scaled', 'wb') as filename:\n",
        "  pickle.dump(s_x_train_scaled, filename)\n",
        "\n",
        "with open('re_s_y_train_scaled', 'wb') as filename:\n",
        "  pickle.dump(s_y_train_scaled, filename)\n",
        "\n",
        "with open('re_s_x_test_scaled', 'wb') as filename:\n",
        "  pickle.dump(s_x_test_scaled, filename)\n",
        "\n",
        "with open('re_s_y_test_scaled', 'wb') as filename:\n",
        "  pickle.dump(s_y_test_scaled, filename)\n",
        "\n",
        "with open('re_x_sScaler', 'wb') as filename:\n",
        "  pickle.dump(x_sScaler, filename)\n",
        "\n",
        "with open('re_y_sScaler', 'wb') as filename:\n",
        "  pickle.dump(y_sScaler, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jLSOxGgfqyg"
      },
      "source": [
        "## Loading ready processed datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umlS9rTI1hTv"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "os.chdir(source_file)\n",
        "\n",
        "with open('re_s_x_train_scaled', 'rb') as filename:\n",
        "  s_x_train_scaled = pickle.load(filename)\n",
        "\n",
        "with open('re_s_y_train_scaled', 'rb') as filename:\n",
        "  s_y_train_scaled = pickle.load(filename)\n",
        "\n",
        "with open('re_s_x_test_scaled', 'rb') as filename:\n",
        "  s_x_test_scaled = pickle.load(filename)\n",
        "\n",
        "with open('re_s_y_test_scaled', 'rb') as filename:\n",
        "  s_y_test_scaled = pickle.load(filename)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC2c9f-6XYIN"
      },
      "source": [
        "## PlainDNN model development"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn-neru9VJb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9083a6fc-ebc8-4e8b-e962-3107762ddac4"
      },
      "source": [
        "# Plain DNN model\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Add, Concatenate, Activation\n",
        "\n",
        "\n",
        "# Define a customised loss function\n",
        "\n",
        "def loss_func(y_true, y_pred): \n",
        "    \n",
        "    # Root mean square loss\n",
        "    squared_difference = tf.square(y_true - y_pred)\n",
        "    mean_square_loss = tf.reduce_mean(squared_difference, axis=-1)\n",
        "    root_mean_square_loss = tf.sqrt(mean_square_loss)\n",
        "    \n",
        "    return root_mean_square_loss \n",
        " \n",
        "def loss_func_with_weights(y_true, y_pred): \n",
        "\n",
        "    alpha = 3.0\n",
        "    beta = 5.0\n",
        "    sq = tf.square(y_true - y_pred)\n",
        "    sq1 = tf.reshape(sq[:, 0], shape=(-1, 1)) \n",
        "    sq2 = tf.reshape(alpha*sq[:, 1], shape=(-1, 1))\n",
        "    sq3 = tf.reshape(beta*sq[:, 2], shape=(-1, 1))\n",
        "    sq4 = Concatenate(axis=-1)([sq1, sq2, sq3])\n",
        "    mean_square_loss = tf.reduce_mean(sq4, axis=-1)\n",
        "    root_mean_square_loss = tf.sqrt(mean_square_loss+1e-20)\n",
        "    \n",
        "    return root_mean_square_loss \n",
        "\n",
        "class PlainDNNModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        \n",
        "        super(PlainDNNModel, self).__init__()\n",
        "        \n",
        "        self.dense1_1 = tf.keras.layers.Dense(796, activation=tf.nn.relu)\n",
        "        self.dense1_2 = tf.keras.layers.Dense(796, activation=tf.nn.relu)\n",
        "        self.dense1_3 = tf.keras.layers.Dense(796, activation=tf.nn.relu)\n",
        "\n",
        "\n",
        "        self.dense2_1 = tf.keras.layers.Dense(796, activation=tf.nn.relu)\n",
        "        self.dense2_2 = tf.keras.layers.Dense(796, activation=tf.nn.relu)\n",
        "        self.dense2_3 = tf.keras.layers.Dense(796, activation=tf.nn.relu)\n",
        "\n",
        "\n",
        "        self.dense3_1 = tf.keras.layers.Dense(796, activation=tf.nn.relu)\n",
        "        self.dense3_2 = tf.keras.layers.Dense(796, activation=tf.nn.relu)\n",
        "        self.dense3_3 = tf.keras.layers.Dense(796, activation=tf.nn.relu)\n",
        "        self.dense3_4 = tf.keras.layers.Dense(796, activation=tf.nn.relu)\n",
        "        self.dense3_o = tf.keras.layers.Dense(3)\n",
        "\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        \n",
        "        x_1 = self.dense1_1(inputs)\n",
        "        x_1 = self.dense1_2(x_1)\n",
        "        x_1 = self.dense1_3(x_1)\n",
        "\n",
        "        \n",
        "        x_2 = self.dense2_1(x_1)\n",
        "        x_2 = self.dense2_2(x_2)\n",
        "        x_2 = self.dense2_3(x_2)\n",
        "\n",
        "        x_3 = self.dense3_1(x_2)\n",
        "        x_3 = self.dense3_2(x_3)\n",
        "        x_3 = self.dense3_3(x_3)\n",
        "        x_3 = self.dense3_4(x_3)\n",
        "        x_3 = self.dense3_o(x_3)\n",
        "        \n",
        "        \n",
        "        return x_3\n",
        "    \n",
        "\n",
        "\n",
        "PlainModel = PlainDNNModel()  \n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True, beta_1=.9, beta_2=.9999)\n",
        "\n",
        "PlainModel.compile(optimizer=opt, loss=loss_func)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-22 07:04:09.000852: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-10-22 07:04:10.617917: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-10-22 07:04:10.620626: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-10-22 07:04:11.137577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-22 07:04:11.138424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-10-22 07:04:11.138472: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-10-22 07:04:11.142830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
            "2021-10-22 07:04:11.142909: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
            "2021-10-22 07:04:11.146692: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-10-22 07:04:11.147297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-10-22 07:04:11.152362: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-10-22 07:04:11.154338: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-10-22 07:04:11.164255: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-10-22 07:04:11.164461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-22 07:04:11.165335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-22 07:04:11.166081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-10-22 07:04:11.166707: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-10-22 07:04:11.169094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-22 07:04:11.169893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-10-22 07:04:11.169936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-10-22 07:04:11.169965: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
            "2021-10-22 07:04:11.169980: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
            "2021-10-22 07:04:11.169995: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-10-22 07:04:11.170010: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-10-22 07:04:11.170025: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-10-22 07:04:11.170041: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-10-22 07:04:11.170056: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-10-22 07:04:11.170177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-22 07:04:11.171016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-22 07:04:11.171800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-10-22 07:04:11.171869: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-10-22 07:04:11.981505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-10-22 07:04:11.981550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-10-22 07:04:11.981561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-10-22 07:04:11.981874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-22 07:04:11.982842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-22 07:04:11.983717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-22 07:04:11.984511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14762 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "2021-10-22 07:04:11.985007: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZmHDFl0RqH1"
      },
      "source": [
        "## Loading pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxktBSzSRpke"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Add, Concatenate, Activation\n",
        "\n",
        "\n",
        "# Define a customised loss function\n",
        "\n",
        "def loss_func(y_true, y_pred): \n",
        "    \n",
        "    # Root mean square loss\n",
        "    squared_difference = tf.square(y_true - y_pred)\n",
        "    mean_square_loss = tf.reduce_mean(squared_difference, axis=-1)\n",
        "    root_mean_square_loss = tf.sqrt(mean_square_loss + 1e-20)\n",
        "    \n",
        "    return root_mean_square_loss \n",
        " \n",
        "def loss_func_with_weights(y_true, y_pred): \n",
        "\n",
        "    alpha = 3.0\n",
        "    beta = 5.0\n",
        "    sq = tf.square(y_true - y_pred)\n",
        "    sq1 = tf.reshape(sq[:, 0], shape=(-1, 1)) \n",
        "    sq2 = tf.reshape(alpha*sq[:, 1], shape=(-1, 1))\n",
        "    sq3 = tf.reshape(beta*sq[:, 2], shape=(-1, 1))\n",
        "    sq4 = Concatenate(axis=-1)([sq1, sq2, sq3])\n",
        "    mean_square_loss = tf.reduce_mean(sq4, axis=-1)\n",
        "    root_mean_square_loss = tf.sqrt(mean_square_loss+1e-20)\n",
        "    \n",
        "    return root_mean_square_loss \n",
        "\n",
        "source_file_model = os.path.join(source_file, 're_HDNNmodel')\n",
        "if os.getcwd != source_file_model:\n",
        "  os.chdir(source_file_model)\n",
        "\n",
        "PlainModel = PlainDNNModel()  \n",
        "PlainModel.load_weights('PlainDNN_3001_noco')\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True, beta_1=.9, beta_2=.9999 )\n",
        "#Hmodel = tf.keras.models.load_model('HDNN_300', compile=False)\n",
        "\n",
        "PlainModel.compile(optimizer=opt, loss=loss_func) # the loss_func is good enough we don't need the loss_func_with_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "dboY7ECuec0h",
        "outputId": "84c07b20-2a70-46ff-85fe-eb2e6c78430a"
      },
      "source": [
        "Hmodel = tf.keras.models.load_model('HDNN_300', compile=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4d2cd16b368a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HDNN_300'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m   raise IOError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    132\u001b[0m   \u001b[0;31m# Recreate layers and metrics using the info stored in the metadata.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m   \u001b[0mkeras_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasObjectLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m   \u001b[0mkeras_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;31m# Generate a dictionary of all loaded nodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload_layers\u001b[0;34m(self, compile)\u001b[0m\n\u001b[1;32m    394\u001b[0m       self.loaded_nodes[node_metadata.node_id] = self._load_layer(\n\u001b[1;32m    395\u001b[0m           \u001b[0mnode_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           node_metadata.metadata)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode_metadata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m_load_layer\u001b[0;34m(self, node_id, identifier, metadata)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_revive_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrevive_custom_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;31m# Add an attribute that stores the extra functions/objects saved in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mrevive_custom_object\u001b[0;34m(identifier, metadata)\u001b[0m\n\u001b[1;32m    985\u001b[0m     revived_cls = type(\n\u001b[1;32m    986\u001b[0m         tf.compat.as_str(metadata['class_name']), parent_classes, {})\n\u001b[0;32m--> 987\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrevived_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m     raise ValueError('Unable to restore custom object of type {} currently. '\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m_init_from_metadata\u001b[0;34m(cls, metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         revived_obj.input_spec = recursively_deserialize_keras_object(\n\u001b[1;32m   1031\u001b[0m             \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_spec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             module_objects={'InputSpec': input_spec.InputSpec})\n\u001b[0m\u001b[1;32m   1033\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'activity_regularizer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         revived_obj.activity_regularizer = regularizers.deserialize(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2775\u001b[0m         hasattr(self.__class__, name)):\n\u001b[1;32m   2776\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTrackable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=bad-super-call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m         raise AttributeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36minput_spec\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1295\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         raise TypeError('Layer input_spec must be an instance of InputSpec. '\n\u001b[0;32m-> 1297\u001b[0;31m                         'Got: {}'.format(v))\n\u001b[0m\u001b[1;32m   1298\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Layer input_spec must be an instance of InputSpec. Got: <keras.initializers.initializers_v2.Zeros object at 0x7f556002e5d0>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMJjyzUGSHye",
        "outputId": "efcf72f6-8c1e-4e95-d013-c068513670b7"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/home/opc/vantai/Datasets_for_magfield/re_HDNNmodel'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ysB5rR7RS2n",
        "outputId": "da789a11-1120-4d02-d4cf-893dd1347442"
      },
      "source": [
        "decay_steps = 40\n",
        "step = [10, 100, 200, 300]\n",
        "decay_rate = 0.5\n",
        "\n",
        "for step in step:\n",
        "  print(decay_rate**(step/decay_steps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8408964152537145\n",
            "0.1767766952966369\n",
            "0.03125\n",
            "0.005524271728019903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es_bd0kRSC6Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1KsduJ5Bsnq",
        "outputId": "72a77e51-b789-40e7-8279-675a67514e9d"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f54e56eba10>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WaHemvYMRZy"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvWFpvzFOKHU",
        "outputId": "dbc1c5d3-fbc3-4418-b6e4-e7aa0c8a6900"
      },
      "source": [
        "import pickle\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime\n",
        "%load_ext tensorboard\n",
        "\n",
        "now = datetime.now()\n",
        "\n",
        "source_file_model = os.path.join(source_file, 're_HDNNmodel')\n",
        "if os.getcwd != source_file_model:\n",
        "  os.chdir(source_file_model)\n",
        "\n",
        "\n",
        "\n",
        "filepath = os.path.join(os.getcwd(), 'PlainDNN_400_796')\n",
        "\n",
        "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor='val_loss', \n",
        "                                                     mode='min', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=20, min_delta=0.00005)\n",
        "\n",
        "new_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', verbose=1, patience=10, \n",
        "                                              factor=1/10, min_lr=1e-5)\n",
        "\n",
        "path = \"logs/fit/\" + now.strftime(\"%Y/%m/%/d, %H/%M\")\n",
        "\n",
        "#tensorboard = tf.keras.callbacks.TensorBoard(log_dir=path, histogram_freq=1)\n",
        "\n",
        "st = time.time()\n",
        "\n",
        "history = PlainModel.fit(s_x_train_scaled, s_y_train_scaled, validation_split=.20, \n",
        "                     batch_size=512, epochs=400, callbacks=[earlystopping, modelcheckpoint, new_lr])\n",
        "\n",
        "used_time = time.time() - st\n",
        "\n",
        "with open('p796_history_400', 'wb') as filename:\n",
        "  pickle.dump(history.history, filename)\n",
        "\n",
        "with open('p796_used_time_400', 'wb') as filename:\n",
        "  pickle.dump(used_time, filename)\n",
        "\n",
        "#oss: 0.1562 - val_loss: 0.1343"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-22 07:04:42.512699: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-10-22 07:04:42.513375: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1999995000 Hz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-22 07:04:43.332088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500/2500 [==============================] - 12s 4ms/step - loss: 0.1765 - val_loss: 0.1000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.10003, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 2/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0956 - val_loss: 0.0774\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.10003 to 0.07741, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 3/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0853 - val_loss: 0.0885\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.07741\n",
            "Epoch 4/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0716 - val_loss: 0.0638\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.07741 to 0.06380, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 5/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0733 - val_loss: 0.0646\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.06380\n",
            "Epoch 6/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0601 - val_loss: 0.0738\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.06380\n",
            "Epoch 7/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0658 - val_loss: 0.0596\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.06380 to 0.05957, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 8/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0573 - val_loss: 0.0567\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.05957 to 0.05669, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 9/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0543 - val_loss: 0.0609\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.05669\n",
            "Epoch 10/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0565 - val_loss: 0.0594\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.05669\n",
            "Epoch 11/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0501 - val_loss: 0.0504\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.05669 to 0.05038, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 12/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0527 - val_loss: 0.0420\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.05038 to 0.04196, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 13/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0470 - val_loss: 0.0452\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.04196\n",
            "Epoch 14/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0496 - val_loss: 0.0548\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.04196\n",
            "Epoch 15/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0480 - val_loss: 0.0556\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.04196\n",
            "Epoch 16/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0481 - val_loss: 0.0478\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.04196\n",
            "Epoch 17/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0456 - val_loss: 0.0425\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.04196\n",
            "Epoch 18/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0449 - val_loss: 0.0369\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.04196 to 0.03688, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 19/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0453 - val_loss: 0.0373\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.03688\n",
            "Epoch 20/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0401 - val_loss: 0.0506\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.03688\n",
            "Epoch 21/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0437 - val_loss: 0.0418\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.03688\n",
            "Epoch 22/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0376 - val_loss: 0.0423\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.03688\n",
            "Epoch 23/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0429 - val_loss: 0.0337\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.03688 to 0.03369, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 24/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0384 - val_loss: 0.0438\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.03369\n",
            "Epoch 25/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0379 - val_loss: 0.0348\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.03369\n",
            "Epoch 26/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0362 - val_loss: 0.0368\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.03369\n",
            "Epoch 27/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0381 - val_loss: 0.0389\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.03369\n",
            "Epoch 28/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0367 - val_loss: 0.0431\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.03369\n",
            "Epoch 29/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0382 - val_loss: 0.0383\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.03369\n",
            "Epoch 30/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0380 - val_loss: 0.0398\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.03369\n",
            "Epoch 31/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0361 - val_loss: 0.0370\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.03369\n",
            "Epoch 32/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0390 - val_loss: 0.0367\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.03369\n",
            "Epoch 33/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0381 - val_loss: 0.0359\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.03369\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 34/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0216 - val_loss: 0.0196\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.03369 to 0.01961, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 35/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0169 - val_loss: 0.0191\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.01961 to 0.01912, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 36/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0160 - val_loss: 0.0191\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.01912 to 0.01911, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 37/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0155 - val_loss: 0.0183\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.01911 to 0.01831, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 38/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0153 - val_loss: 0.0185\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.01831\n",
            "Epoch 39/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0150 - val_loss: 0.0177\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.01831 to 0.01770, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 40/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0142 - val_loss: 0.0177\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.01770 to 0.01767, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 41/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0143 - val_loss: 0.0176\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.01767 to 0.01759, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 42/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0140 - val_loss: 0.0173\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.01759 to 0.01730, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 43/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0137 - val_loss: 0.0171\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.01730 to 0.01712, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 44/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0139 - val_loss: 0.0171\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.01712 to 0.01706, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 45/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0133 - val_loss: 0.0174\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01706\n",
            "Epoch 46/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0134 - val_loss: 0.0171\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01706\n",
            "Epoch 47/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0138 - val_loss: 0.0167\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.01706 to 0.01674, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 48/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0130 - val_loss: 0.0167\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.01674 to 0.01672, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 49/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0127 - val_loss: 0.0163\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.01672 to 0.01626, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 50/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0126 - val_loss: 0.0160\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.01626 to 0.01600, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 51/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0125 - val_loss: 0.0161\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01600\n",
            "Epoch 52/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0124 - val_loss: 0.0165\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01600\n",
            "Epoch 53/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0123 - val_loss: 0.0165\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01600\n",
            "Epoch 54/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0122 - val_loss: 0.0159\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.01600 to 0.01587, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 55/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0119 - val_loss: 0.0159\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01587\n",
            "Epoch 56/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0123 - val_loss: 0.0165\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01587\n",
            "Epoch 57/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0121 - val_loss: 0.0160\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01587\n",
            "Epoch 58/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0120 - val_loss: 0.0159\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01587\n",
            "Epoch 59/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0116 - val_loss: 0.0155\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.01587 to 0.01547, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 60/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0117 - val_loss: 0.0168\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.01547\n",
            "Epoch 61/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0117 - val_loss: 0.0154\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.01547 to 0.01540, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 62/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0114 - val_loss: 0.0156\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.01540\n",
            "Epoch 63/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0115 - val_loss: 0.0153\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.01540 to 0.01531, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 64/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0120 - val_loss: 0.0155\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.01531\n",
            "Epoch 65/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0114 - val_loss: 0.0151\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.01531 to 0.01514, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 66/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0115 - val_loss: 0.0151\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.01514 to 0.01513, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 67/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0113 - val_loss: 0.0153\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.01513\n",
            "Epoch 68/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0114 - val_loss: 0.0158\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.01513\n",
            "Epoch 69/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0111 - val_loss: 0.0153\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.01513\n",
            "Epoch 70/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0111 - val_loss: 0.0156\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.01513\n",
            "Epoch 71/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0112 - val_loss: 0.0150\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.01513 to 0.01497, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 72/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0113 - val_loss: 0.0155\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.01497\n",
            "Epoch 73/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0107 - val_loss: 0.0152\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.01497\n",
            "Epoch 74/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0110 - val_loss: 0.0152\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.01497\n",
            "Epoch 75/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0110 - val_loss: 0.0152\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.01497\n",
            "Epoch 76/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0107 - val_loss: 0.0147\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.01497 to 0.01469, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 77/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0107 - val_loss: 0.0149\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.01469\n",
            "Epoch 78/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0105 - val_loss: 0.0150\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01469\n",
            "Epoch 79/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0105 - val_loss: 0.0146\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.01469 to 0.01458, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 80/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0104 - val_loss: 0.0147\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01458\n",
            "Epoch 81/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0106 - val_loss: 0.0149\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01458\n",
            "Epoch 82/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0104 - val_loss: 0.0149\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01458\n",
            "Epoch 83/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0102 - val_loss: 0.0146\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.01458 to 0.01456, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 84/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0101 - val_loss: 0.0146\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01456\n",
            "Epoch 85/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0101 - val_loss: 0.0146\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.01456 to 0.01455, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 86/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0101 - val_loss: 0.0160\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01455\n",
            "Epoch 87/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0107 - val_loss: 0.0145\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.01455 to 0.01449, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 88/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0100 - val_loss: 0.0147\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01449\n",
            "Epoch 89/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0102 - val_loss: 0.0143\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.01449 to 0.01430, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 90/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0101 - val_loss: 0.0145\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01430\n",
            "Epoch 91/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0101 - val_loss: 0.0143\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.01430 to 0.01430, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 92/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0111 - val_loss: 0.0145\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01430\n",
            "Epoch 93/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0101 - val_loss: 0.0145\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01430\n",
            "Epoch 94/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0101 - val_loss: 0.0141\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.01430 to 0.01411, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 95/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0100 - val_loss: 0.0142\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01411\n",
            "Epoch 96/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0100 - val_loss: 0.0141\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01411\n",
            "Epoch 97/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0101 - val_loss: 0.0142\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01411\n",
            "Epoch 98/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0098 - val_loss: 0.0141\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01411\n",
            "Epoch 99/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0097 - val_loss: 0.0144\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01411\n",
            "Epoch 100/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0098 - val_loss: 0.0143\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01411\n",
            "Epoch 101/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0099 - val_loss: 0.0140\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.01411 to 0.01402, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 102/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0101 - val_loss: 0.0146\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.01402\n",
            "Epoch 103/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0096 - val_loss: 0.0141\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.01402\n",
            "Epoch 104/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0095 - val_loss: 0.0141\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.01402\n",
            "\n",
            "Epoch 00104: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 105/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0089 - val_loss: 0.0137\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.01402 to 0.01368, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 106/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0088 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.01368 to 0.01363, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 107/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0088 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.01363\n",
            "Epoch 108/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0087 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.01363 to 0.01362, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 109/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0087 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.01362\n",
            "Epoch 110/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0087 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.01362 to 0.01361, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 111/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0087 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.01361 to 0.01361, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 112/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0087 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.01361 to 0.01360, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 113/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.01360\n",
            "Epoch 114/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0087 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.01360 to 0.01358, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 115/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0087 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.01358\n",
            "\n",
            "Epoch 00115: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 116/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0087 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.01358 to 0.01358, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 117/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0086 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.01358\n",
            "Epoch 118/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0086 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.01358\n",
            "Epoch 119/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.01358 to 0.01356, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 120/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0087 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.01356\n",
            "Epoch 121/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0086 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.01356\n",
            "Epoch 122/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.01356\n",
            "Epoch 123/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.01356 to 0.01356, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 124/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.01356\n",
            "Epoch 125/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.01356 to 0.01355, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 126/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0086 - val_loss: 0.0136\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.01355\n",
            "Epoch 127/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.01355 to 0.01354, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 128/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.01354\n",
            "Epoch 129/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.01354 to 0.01353, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 130/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.01353\n",
            "Epoch 131/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.01353\n",
            "Epoch 132/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.01353\n",
            "Epoch 133/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0085 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.01353 to 0.01352, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 134/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0085 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.01352\n",
            "Epoch 135/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.01352 to 0.01352, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 136/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0085 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.01352\n",
            "Epoch 137/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0086 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.01352 to 0.01352, saving model to /home/opc/vantai/Datasets_for_magfield/re_HDNNmodel/PlainDNN_400_796\n",
            "Epoch 138/400\n",
            "2500/2500 [==============================] - 11s 4ms/step - loss: 0.0086 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.01352\n",
            "Epoch 139/400\n",
            "2500/2500 [==============================] - 10s 4ms/step - loss: 0.0085 - val_loss: 0.0135\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.01352\n",
            "Epoch 00139: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLASRK15lsYo",
        "outputId": "9680778b-63d0-4470-96d3-d76a84ed935d"
      },
      "source": [
        "PlainModel.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"plain_dnn_model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_110 (Dense)            multiple                  7960      \n",
            "_________________________________________________________________\n",
            "dense_111 (Dense)            multiple                  634412    \n",
            "_________________________________________________________________\n",
            "dense_112 (Dense)            multiple                  634412    \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            multiple                  634412    \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            multiple                  634412    \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            multiple                  634412    \n",
            "_________________________________________________________________\n",
            "dense_116 (Dense)            multiple                  634412    \n",
            "_________________________________________________________________\n",
            "dense_117 (Dense)            multiple                  634412    \n",
            "_________________________________________________________________\n",
            "dense_118 (Dense)            multiple                  634412    \n",
            "_________________________________________________________________\n",
            "dense_119 (Dense)            multiple                  634412    \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            multiple                  2391      \n",
            "=================================================================\n",
            "Total params: 5,720,059\n",
            "Trainable params: 5,720,059\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "nRD7e9UPeVgG",
        "outputId": "4e9ad19c-7ba1-42c8-e143-f49fdc323242"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 27511), started 0:01:47 ago. (Use '!kill 27511' to kill it.)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-a3df65dbff625775\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-a3df65dbff625775\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u475odLYG8Ds"
      },
      "source": [
        "## Model testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad1zBGPzWhmq"
      },
      "source": [
        "# Predicting the results\n",
        "s_y_predicted_scaled = PlainModel.predict(s_x_test_scaled)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIzkXcOSxOfT",
        "outputId": "9da5a3c3-3a49-43af-8b14-6b4f0266088d"
      },
      "source": [
        "# Testing the model\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "r2_score(s_y_test_scaled[:, 2], s_y_predicted_scaled[:, 2])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9743515739083507"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8dBFhaFUQJT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}